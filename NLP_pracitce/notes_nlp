# Formatting a lil

print(f"{name:{10}} {place:{10}} {thing:.>{10}}")
Shaan      Konoha     .....Kunai

DateTime formatting
>>> datetime.datetime(2019, 7, 26, 0, 0)
f"{today}"
'2019-07-26 00:00:00'

f"{today:%B %d %y}"
'July 26 19'


# Explore Spacy

import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Spacy will create tokens out of this and much more!!!")


# Scikit Learn

Labels and Features
True Positive - Ham message is positive and predicted successfully
True Negative - Spam message is negative and predicted successfully
false Positive - Ham message is positive and predicted unsucessfully
False Negative - Spam message is positive and predicted unsucessfully

Classification Matrix

Accuracy - No of correct predicitions made by the model divided by the total number
eg - 80 correct out of 100 gives an  accuracy of 80/100 = 0.8
TP + TN / Total No
Error Rate -  FP + FN / Total

Recall - No of true positive / no of true positive + no of false negatives

Precision -> True Pos / true pos + false pos

F1 - Score -> 2 * precision * recall
                  ------------------
                  precision + recall

Confusion Matrix
https://en.wikipedia.org/wiki/Confusion_matrix

In classifications we will have 2 conditions
1.) True Condition
2.) Predicted Condition


SCIKIT LEARN
ML Process
                         ----------------Test Data-------------------
                        |                                            |
Data Acquisition -> Data Cleaning -> Model Training and Building -> Model Testing -> Model Deployment
                                                 |                   |
                                                 |--------------------



TOPIC MODELING

Allows for us to efficiently analyze larg volumes of text by
 clustering documents into topics.


Latent Dirichlet Allocation - LDA
Documents with similar topics use similar group of words
Latent topics can then be found b searching groups of words that frequently occur
together in documents across the corpus


PERCEPTRON

Input 0(12)------Weight 0(12 * 0.5 = 6) ---------
                                    ((Activation Function)) --------------> Output ( 1 )
Input 1(4) ------Weight 1 (4 * -1 = -4 )---------

Bias --------Weight +1 ------------------------


There area many types of activation Functions
for eg if the sum of the above input is > 0 then output is 1
else  0
so for above its 1

Bias Terms - If the input is 0

n
E w(i)x(i) + b
i=0


Neural Networks

Input Layer    {hidden layer}        Output Layer

O -----                    O -------\
            O -------                \
O ------                   O -------- O
            O -------                 /
O ------                   O --------/

If we have 3 or more hidden layers then its a deep network

1|   |-----
 |   |
 |----
0|__________

z = wx + b


2.) Another activation function is sigmoid function

f(x) = 1
     -----
     1 + -(x)
         e
and we can replace x by z
so x becomes wx + b


3.) Hyperbolic Tangent: tanh(z)
in this output is from  -1 to 1

         sinh x    e(x) - e(-x)
tanh x = ------ =  ------------
         cosh x     e(x) + e(-x)


4.) ReLU Rectified Linear Unit ( best performance in many situations)

max(0, z) = max ( 0 , wx + b)




RECURRENT Neural Network

Works best on sequence data such as
sales data
Audio
Sentences
music

A RNN sends the output back to itself....as a feedback?

output at t-1       output at t           output at t + 1
   /|\                  /|\                     /|\
    | output             |                       |
    O ------->           O ------->              O ------->
   /|\                  /|\                     /|\
    |                    |                       |
input at t - 1         input at t           input at t + 1


-------------------------------------------------------------------->
                    TIME --->
Each set of neuron has 2 inputs
1.) one as a feedback (memory cells)
2.) a new one

So this was for just one neuron....the same thing can be done with any neurons


   x ---------- O    --------
x ----------  O   --------  y  ------->
  x ---------- O    --------|
         /|\                |
          |__________________
            to all the three



LSTM - Long Short Term Memory Units -- for text generation
GRU - Gated Recurrent Unit